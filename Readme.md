Inside this folder, you can find some subfolders.

- training-code: this folder contains the code for training the model created on ModelArts. To use this script (trainin.py) it is necessary to specify some parameters, the three datasets (training_set, validation_set and test_set) that are used for training, the path to the labels and a configuration file that contains some specifications for the model that can be changed. The three datasets are a preprocessed version of the synthetically generated image dataset. For using this code, a bucket was first set up on the OBS service and then an algorithm was created, kicked off via trainin-jobs. Instead, the datasets were placed in a second bucket, the one called dataset. It is also necessary to specify an output folder, in this case it is the training-output folder for which a bucket with the same name has been set up. The output of a training-job started using this algorithm can be found in this bucket. You can find a the training-video that explain the steps.

- dataset: the dataset used for training in the experiments started with training-jobs on ModelArts

- training-output: as mentioned, the output of a training-job flows into the training-output folder located in an OBS bucket with the same name. In this folder you can find the python script to add to the same folder on the cloud in the bucket to start the deployment of the model on a real-time server. Also you can find a configs.json file which contains the specifics for the server to start. To carry out this operation, AI applications have been created and used to deploy the real-time server. The deploy-video show the process.

- APIGW-python-sdk-2.0.4: contains a simple code to do inference inside main.py. As indicated in the presentation Mediapipe (Google) is run locally and the landmarks obtained are sent to the cloud through a special call to perform the inference and return the aforementioned gesture locally.

- hands-capture: for the creation of the synthetic dataset, on the Windows server created on the ECS service, it is first necessary to make some acquisitions of the hands, to generate files called Pose.json containing the joints of the hands, these are then used for the generation of synthetic hands. In this folder there is the script that leads to the creation of these json files. There are some parameters that allow you to change the capture configurations.

- dataset-generator: an ECS service was used to start a Windows Server 2019 machine. Here the code server-side for generating the synthetic dataset was implemented. The Pose.json files have already been manually uploaded to the machine (This part has not yet been automated). Here a web server has been started which allows access to the generation of the dataset in an elementary way. By accessing the following url, you are directed to a web page. A Start button is available here. Clicked, a process starts on the machine which leads to the generation of the synthetic dataset and its download with a Download button which appears at the end of the generation operations. The operation may take a few minutes. The dataset-generator-video shows what happens on the server. A user will simply see a download button appear. This is the url

     http://119.8.214.248:8080/test/

- apk: this folder contains an android application created by our team that reflects the final functioning of the service on a smartphone. In this case the recognized gestures can be made with the camera behind, making gestures in front of the camera, with both hands and at the same time